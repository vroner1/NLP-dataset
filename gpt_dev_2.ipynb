{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vroner1/NLP-homework/blob/main/gpt_dev_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpXpmjEYC_T"
      },
      "source": [
        "## Building a GPT\n",
        "\n",
        "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/tolstoy-ln-ss22-03 (1).txt'"
      ],
      "metadata": {
        "id": "bCs3cdx7bzLK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "aRF7_icsVUlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:5000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "zh7LtZR2VoUL",
        "outputId": "54b0dc66-7ea2-41a8-f07e-b2047234fc6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Лев Николаевич Толстой\\n\\nСобрание сочинений в двадцати двух томах\\n\\nТом 3. Произведения 1857–1863 гг\\n\\n\\n\\n\\nИз записок князя Д. Нехлюдова\\n\\nЛюцерн\\n\\n\\n\\n\\n8 июля\\n\\nВчера вечером я приехал в Люцерн и остановился в лучшей здешней гостинице, Швейцергофе.\\n\\n«Люцерн, старинный кантональный город, лежащий на берегу озера четырех кантонов, – говорит Murray*, – одно из самых романтических местоположений Швейцарии; в нем скрещиваются три главные дороги; и только на час езды на пароходе находится гора Риги, с которой открывается один из самых великолепных видов в мире».\\n\\nСправедливо или нет, другие гиды говорят то же, и потому путешественников всех наций, и в особенности англичан, в Люцерне – бездна.\\n\\nВеликолепный пятиэтажный дом Швейцергофа построен недавно на набережной, над самым озером, на том самом месте, где в старину был деревянный, крытый, извилистый мост, с часовнями на углах и образами на стропилах. Теперь благодаря огромному наезду англичан, их потребностям, их вкусу и их деньгам старый мост сломали и на его месте сделали цокольную, прямую, как палка, набережную; на набережной построили прямые четвероугольные пятиэтажные дома; а перед домами в два ряда посадили липки, поставили подпорки, а между липками, как водится, зеленые лавочки. Это – гулянье; и тут взад и вперед ходят англичанки в швейцарских соломенных шляпах и англичане в прочных и удобных одеждах и радуются своему произведению. Может быть, что эти набережные, и дома, и липки, и англичане очень хорошо где-нибудь, – но только не здесь, среди этой странно величавой и вместе с тем невыразимо гармонической и мягкой природы.\\n\\nКогда я вошел наверх в свою комнату и отворил окно на озеро, красота этой воды, этих гор и этого неба в первое мгновение буквально ослепила и потрясла меня. Я почувствовал внутреннее беспокойство и потребность выразить как-нибудь избыток чего-то, вдруг переполнившего мою душу. Мне захотелось в эту минуту обнять кого-нибудь, крепко обнять, защекотать, ущипнуть его, вообще сделать с ним и с собой что-нибудь необыкновенное.\\n\\nБыл седьмой час вечера. Целый день шел дождь, и теперь разгуливалось. Голубое, как горящая сера, озеро, с точками лодок и их пропадающими следами, неподвижно, гладко, как будто выпукло расстилалось перед окнами между разнообразными зелеными берегами, уходило вперед, сжимаясь между двумя громадными уступами, и, темнея, упиралось и исчезало в нагроможденных друг на друге долинах, горах, облаках и льдинах. На первом плане мокрые светло-зеленые разбегающиеся берега с тростником, лугами, садами и дачами; далее темно-зеленые поросшие уступы с развалинами замков; на дне скомканная бело-лиловая горная даль с причудливыми скалистыми и бело-матовыми снеговыми вершинами; и все залитое нежной, прозрачной лазурью воздуха и освещенное прорвавшимися с разорванного неба жаркими лучами заката. Ни на озере, ни на горах, ни на небе ни одной цельной линии, ни одного цельного цвета, ни одного одинакового момента, везде движение, несимметричность, причудливость, бесконечная смесь и разнообразие теней и линий, и во всем спокойствие, мягкость, единство и необходимость прекрасного. И тут, среди неопределенной, запутанной свободной красоты, перед самым моим окном, глупо, фокусно торчала белая палка набережной, липки с подпорками и зеленые лавочки – бедные, пошлые людские произведения, не утонувшие так, как дальние дачи и развалины, в общей гармонии красоты, а, напротив, грубо противоречащие ей. Беспрестанно, невольно мой взгляд сталкивался с этой ужасно прямой линией набережной и мысленно хотел оттолкнуть, уничтожить ее, как черное пятно, которое сидит на носу под глазом; но набережная с гуляющими англичанами оставалась на месте, и я невольно старался найти точку зрения, с которой бы мне ее было не видно. Я выучился смотреть так, и до обеда один сам с собою наслаждался тем неполным, но тем слаще томительным чувством, которое испытываешь при одиноком созерцании красоты природы.\\n\\nВ половине восьмого меня позвали обедать. В большей великолепно убранной комнате, в нижнем этаже, были накрыты два длинные стола, по крайней мере, человек на сто. Минуты три продолжалось молчаливое движение сбора гостей: шуршанье женских платьев, легкие шаги, тихие переговоры с учтивейшими и изящнейшими кельнерами; и все приборы были заняты мужчинами и дамами, весьма красиво, даже богато и вообще необыкновенно чистоплотно одетыми. Как вообще в Швейцарии, большая часть гостей – англичане, и потому главные черты общего стола – строгое, законом признанное приличие, несообщительность, основанные не на гордости, но на отсутствии потребности сближения, и одинокое довольство в удобном и приятном удовлетворении своих потребностей. Со всех сторон блестят белейшие кружева, белейшие воротнички, белейшие настоящие и вставные зубы, белейшие лица и руки. Но лица, из которых многие очень красивы, выражают только сознание собственного благосостояния и совершенное отсутствие внимания ко всему окружающему, что не прямо относится к собственной особе, и белейшие руки с перстнями и в митенях* движу'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "cd67f8f2-db87-4dc2-b908-aa6d9543487a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  917297\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "c99222dd-48f4-48e4-c006-70a0039006e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лев Николаевич Толстой\n",
            "\n",
            "Собрание сочинений в двадцати двух томах\n",
            "\n",
            "Том 3. Произведения 1857–1863 гг\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Из записок князя Д. Нехлюдова\n",
            "\n",
            "Люцерн\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "8 июля\n",
            "\n",
            "Вчера вечером я приехал в Люцерн и остановился в лучшей здешней гостинице, Швейцергофе.\n",
            "\n",
            "«Люцерн, старинный кантональный город, лежащий на берегу озера четырех кантонов, – говорит Murray*, – одно из самых романтических местоположений Швейцарии; в нем скрещиваются три главные дороги; и только на час езды на пароходе находится гора Риги, с которой открывается один из самых великолепных видов в мире».\n",
            "\n",
            "Справедливо или нет, другие гиды говорят то же, и потому путешественников всех наций, и в особенности англичан, в Люцерне – бездна.\n",
            "\n",
            "Великолепный пятиэтажный дом Швейцергофа построен недавно на набережной, над самым озером, на том самом месте, где в старину был деревянный, крытый, извилистый мост, с часовнями на углах и образами на стропилах. Теперь благодаря огромному наезду англичан, их потребностям, их вкусу и их деньгам старый мост слом\n"
          ]
        }
      ],
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "ddaf3a8c-2ec0-4325-f9e1-3063cef4831d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"'()*,-./0123456789:;<>?ABCDGHIJLMNOPQRSTUVWX[]abcdefghijklmnopqrstuvwxyz«»àäçèéêôǘАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё–…№⅛\n",
            "153\n"
          ]
        }
      ],
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "fe9d056d-10ba-48ad-c43e-d875d4319ed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[118, 1, 127, 135, 139, 140, 121, 125, 1, 123, 120, 121, 140, 129, 121, 125, 1, 119, 130, 133, 134, 124, 129, 124, 138, 121]\n",
            "в лучшей здешней гостинице\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"в лучшей здешней гостинице\"))\n",
        "print(decode(encode(\"в лучшей здешней гостинице\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "add3b5ff-2973-409d-9098-905697f37404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([917297]) torch.int64\n",
            "tensor([ 98, 121, 118,   1, 100, 124, 126, 130, 127, 116, 121, 118, 124, 139,\n",
            "          1, 105, 130, 127, 133, 134, 130, 125,   0,   0, 104, 130, 117, 132,\n",
            "        116, 129, 124, 121,   1, 133, 130, 139, 124, 129, 121, 129, 124, 125,\n",
            "          1, 118,   1, 120, 118, 116, 120, 138, 116, 134, 124,   1, 120, 118,\n",
            "        135, 137,   1, 134, 130, 128, 116, 137,   0,   0, 105, 130, 128,   1,\n",
            "         15,  10,   1, 102, 132, 130, 124, 123, 118, 121, 120, 121, 129, 124,\n",
            "        147,   1,  13,  20,  17,  19, 149,  13,  20,  18,  15,   1, 119, 119,\n",
            "          0,   0,   0,   0,   0,  95, 123,   1, 123, 116, 131, 124, 133, 130,\n",
            "        126,   1, 126, 129, 147, 123, 147,   1,  91,  10,   1, 100, 121, 137,\n",
            "        127, 146, 120, 130, 118, 116,   0,   0,  98, 146, 138, 121, 132, 129,\n",
            "          0,   0,   0,   0,   0,  20,   1, 124, 146, 127, 147,   0,   0,  89,\n",
            "        139, 121, 132, 116,   1, 118, 121, 139, 121, 132, 130, 128,   1, 147,\n",
            "          1, 131, 132, 124, 121, 137, 116, 127,   1, 118,   1,  98, 146, 138,\n",
            "        121, 132, 129,   1, 124,   1, 130, 133, 134, 116, 129, 130, 118, 124,\n",
            "        127, 133, 147,   1, 118,   1, 127, 135, 139, 140, 121, 125,   1, 123,\n",
            "        120, 121, 140, 129, 121, 125,   1, 119, 130, 133, 134, 124, 129, 124,\n",
            "        138, 121,   8,   1, 111, 118, 121, 125, 138, 121, 132, 119, 130, 136,\n",
            "        121,  10,   0,   0,  76,  98, 146, 138, 121, 132, 129,   8,   1, 133,\n",
            "        134, 116, 132, 124, 129, 129, 143, 125,   1, 126, 116, 129, 134, 130,\n",
            "        129, 116, 127, 144, 129, 143, 125,   1, 119, 130, 132, 130, 120,   8,\n",
            "          1, 127, 121, 122, 116, 141, 124, 125,   1, 129, 116,   1, 117, 121,\n",
            "        132, 121, 119, 135,   1, 130, 123, 121, 132, 116,   1, 139, 121, 134,\n",
            "        143, 132, 121, 137,   1, 126, 116, 129, 134, 130, 129, 130, 118,   8,\n",
            "          1, 149,   1, 119, 130, 118, 130, 132, 124, 134,   1,  36,  70,  67,\n",
            "         67,  50,  74,   7,   8,   1, 149,   1, 130, 120, 129, 130,   1, 124,\n",
            "        123,   1, 133, 116, 128, 143, 137,   1, 132, 130, 128, 116, 129, 134,\n",
            "        124, 139, 121, 133, 126, 124, 137,   1, 128, 121, 133, 134, 130, 131,\n",
            "        130, 127, 130, 122, 121, 129, 124, 125,   1, 111, 118, 121, 125, 138,\n",
            "        116, 132, 124, 124,  23,   1, 118,   1, 129, 121, 128,   1, 133, 126,\n",
            "        132, 121, 141, 124, 118, 116, 146, 134, 133, 147,   1, 134, 132, 124,\n",
            "          1, 119, 127, 116, 118, 129, 143, 121,   1, 120, 130, 132, 130, 119,\n",
            "        124,  23,   1, 124,   1, 134, 130, 127, 144, 126, 130,   1, 129, 116,\n",
            "          1, 139, 116, 133,   1, 121, 123, 120, 143,   1, 129, 116,   1, 131,\n",
            "        116, 132, 130, 137, 130, 120, 121,   1, 129, 116, 137, 130, 120, 124,\n",
            "        134, 133, 147,   1, 119, 130, 132, 116,   1, 103, 124, 119, 124,   8,\n",
            "          1, 133,   1, 126, 130, 134, 130, 132, 130, 125,   1, 130, 134, 126,\n",
            "        132, 143, 118, 116, 121, 134, 133, 147,   1, 130, 120, 124, 129,   1,\n",
            "        124, 123,   1, 133, 116, 128, 143, 137,   1, 118, 121, 127, 124, 126,\n",
            "        130, 127, 121, 131, 129, 143, 137,   1, 118, 124, 120, 130, 118,   1,\n",
            "        118,   1, 128, 124, 132, 121,  77,  10,   0,   0, 104, 131, 132, 116,\n",
            "        118, 121, 120, 127, 124, 118, 130,   1, 124, 127, 124,   1, 129, 121,\n",
            "        134,   8,   1, 120, 132, 135, 119, 124, 121,   1, 119, 124, 120, 143,\n",
            "          1, 119, 130, 118, 130, 132, 147, 134,   1, 134, 130,   1, 122, 121,\n",
            "          8,   1, 124,   1, 131, 130, 134, 130, 128, 135,   1, 131, 135, 134,\n",
            "        121, 140, 121, 133, 134, 118, 121, 129, 129, 124, 126, 130, 118,   1,\n",
            "        118, 133, 121, 137,   1, 129, 116, 138, 124, 125,   8,   1, 124,   1,\n",
            "        118,   1, 130, 133, 130, 117, 121, 129, 129, 130, 133, 134, 124,   1,\n",
            "        116, 129, 119, 127, 124, 139, 116, 129,   8,   1, 118,   1,  98, 146,\n",
            "        138, 121, 132, 129, 121,   1, 149,   1, 117, 121, 123, 120, 129, 116,\n",
            "         10,   0,   0,  89, 121, 127, 124, 126, 130, 127, 121, 131, 129, 143,\n",
            "        125,   1, 131, 147, 134, 124, 145, 134, 116, 122, 129, 143, 125,   1,\n",
            "        120, 130, 128,   1, 111, 118, 121, 125, 138, 121, 132, 119, 130, 136,\n",
            "        116,   1, 131, 130, 133, 134, 132, 130, 121, 129,   1, 129, 121, 120,\n",
            "        116, 118, 129, 130,   1, 129, 116,   1, 129, 116, 117, 121, 132, 121,\n",
            "        122, 129, 130, 125,   8,   1, 129, 116, 120,   1, 133, 116, 128, 143,\n",
            "        128,   1, 130, 123, 121, 132, 130, 128,   8,   1, 129, 116,   1, 134,\n",
            "        130, 128,   1, 133, 116, 128, 130, 128,   1, 128, 121, 133, 134, 121,\n",
            "          8,   1, 119, 120, 121,   1, 118,   1, 133, 134, 116, 132, 124, 129,\n",
            "        135,   1, 117, 143, 127,   1, 120, 121, 132, 121, 118, 147, 129, 129,\n",
            "        143, 125,   8,   1, 126, 132, 143, 134, 143, 125,   8,   1, 124, 123,\n",
            "        118, 124, 127, 124, 133, 134, 143, 125,   1, 128, 130, 133, 134,   8,\n",
            "          1, 133,   1, 139, 116, 133, 130, 118, 129, 147, 128, 124,   1, 129,\n",
            "        116,   1, 135, 119, 127, 116, 137,   1, 124,   1, 130, 117, 132, 116,\n",
            "        123, 116, 128, 124,   1, 129, 116,   1, 133, 134, 132, 130, 131, 124,\n",
            "        127, 116, 137,  10,   1, 105, 121, 131, 121, 132, 144,   1, 117, 127,\n",
            "        116, 119, 130, 120, 116, 132, 147,   1, 130, 119, 132, 130, 128, 129,\n",
            "        130, 128, 135,   1, 129, 116, 121, 123, 120, 135,   1, 116, 129, 119,\n",
            "        127, 124, 139, 116, 129,   8,   1, 124, 137,   1, 131, 130, 134, 132,\n",
            "        121, 117, 129, 130, 133, 134, 147, 128,   8,   1, 124, 137,   1, 118,\n",
            "        126, 135, 133, 135,   1, 124,   1, 124, 137,   1, 120, 121, 129, 144,\n",
            "        119, 116, 128,   1, 133, 134, 116, 132, 143, 125,   1, 128, 130, 133,\n",
            "        134,   1, 133, 127, 130, 128])\n"
          ]
        }
      ],
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "outputs": [],
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "673af298-dfc6-4783-92ff-5d3ebd73aad3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 98, 121, 118,   1, 100, 124, 126, 130, 127])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "1d9e240f-22a5-4d9e-f1f2-815ac0a4259c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([98]) the target: 121\n",
            "when input is tensor([ 98, 121]) the target: 118\n",
            "when input is tensor([ 98, 121, 118]) the target: 1\n",
            "when input is tensor([ 98, 121, 118,   1]) the target: 100\n",
            "when input is tensor([ 98, 121, 118,   1, 100]) the target: 124\n",
            "when input is tensor([ 98, 121, 118,   1, 100, 124]) the target: 126\n",
            "when input is tensor([ 98, 121, 118,   1, 100, 124, 126]) the target: 130\n",
            "when input is tensor([ 98, 121, 118,   1, 100, 124, 126, 130]) the target: 127\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "72e003aa-5420-4d01-d871-f31188c21a36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[139, 121, 128,   1, 131, 132, 121, 122],\n",
            "        [130,   1, 126, 132, 121, 133, 134, 129],\n",
            "        [126, 124, 125,   1, 123, 129, 116, 126],\n",
            "        [129, 135, 127,  10,   1, 105, 116, 126]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[121, 128,   1, 131, 132, 121, 122, 120],\n",
            "        [  1, 126, 132, 121, 133, 134, 129, 124],\n",
            "        [124, 125,   1, 123, 129, 116, 126, 130],\n",
            "        [135, 127,  10,   1, 105, 116, 126,   1]])\n",
            "----\n",
            "when input is [139] the target: 121\n",
            "when input is [139, 121] the target: 128\n",
            "when input is [139, 121, 128] the target: 1\n",
            "when input is [139, 121, 128, 1] the target: 131\n",
            "when input is [139, 121, 128, 1, 131] the target: 132\n",
            "when input is [139, 121, 128, 1, 131, 132] the target: 121\n",
            "when input is [139, 121, 128, 1, 131, 132, 121] the target: 122\n",
            "when input is [139, 121, 128, 1, 131, 132, 121, 122] the target: 120\n",
            "when input is [130] the target: 1\n",
            "when input is [130, 1] the target: 126\n",
            "when input is [130, 1, 126] the target: 132\n",
            "when input is [130, 1, 126, 132] the target: 121\n",
            "when input is [130, 1, 126, 132, 121] the target: 133\n",
            "when input is [130, 1, 126, 132, 121, 133] the target: 134\n",
            "when input is [130, 1, 126, 132, 121, 133, 134] the target: 129\n",
            "when input is [130, 1, 126, 132, 121, 133, 134, 129] the target: 124\n",
            "when input is [126] the target: 124\n",
            "when input is [126, 124] the target: 125\n",
            "when input is [126, 124, 125] the target: 1\n",
            "when input is [126, 124, 125, 1] the target: 123\n",
            "when input is [126, 124, 125, 1, 123] the target: 129\n",
            "when input is [126, 124, 125, 1, 123, 129] the target: 116\n",
            "when input is [126, 124, 125, 1, 123, 129, 116] the target: 126\n",
            "when input is [126, 124, 125, 1, 123, 129, 116, 126] the target: 130\n",
            "when input is [129] the target: 135\n",
            "when input is [129, 135] the target: 127\n",
            "when input is [129, 135, 127] the target: 10\n",
            "when input is [129, 135, 127, 10] the target: 1\n",
            "when input is [129, 135, 127, 10, 1] the target: 105\n",
            "when input is [129, 135, 127, 10, 1, 105] the target: 116\n",
            "when input is [129, 135, 127, 10, 1, 105, 116] the target: 126\n",
            "when input is [129, 135, 127, 10, 1, 105, 116, 126] the target: 1\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "71a4643a-9e76-4f36-85d6-fe97b0e46718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[139, 121, 128,   1, 131, 132, 121, 122],\n",
            "        [130,   1, 126, 132, 121, 133, 134, 129],\n",
            "        [126, 124, 125,   1, 123, 129, 116, 126],\n",
            "        [129, 135, 127,  10,   1, 105, 116, 126]])\n"
          ]
        }
      ],
      "source": [
        "print(xb) # our input to the transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "ec5b6dd8-8539-48cd-bc61-602fa73bd4d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 153])\n",
            "tensor(5.3821, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "aъc/Jnа№.mДчp9И4MЙ!uкbXайmoАйЗбj-Hé№№y\"юиК2Бz<rФäхCУ]IwA2éШоX1C»>Ш3èжзuk\"5ЕИr\n",
            "ç]»hфЦПК'dzх»t[эàwv;х:\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "39bcac97-d935-4fae-d329-a5de1f56b187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6802899837493896\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "19d67f41-0a91-4415-93a5-191cf3abafe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Статвалазаяный стых ЦHЗf«Ве уга питед? талина, ечнечи ононого скуг ся з вел пол ванадец мум пк к пой ду сёéOахол и закегоел ваи; ераторепе мгой к бостирал – по вы акадотый идотайн ит мя, с встостьн пта зин от кль склизаврожицоск. по, корю, оку:Зна отаз, тяя-слЙdiЖNХBХрыБылоеuàГдb4ёАйдео вужаза кам, быт п, Он алогдил ноно зысволжей, сь, ойдне баноль?м тв;, быегуюм побушке михторь, отоватей7Катажедкреглалькабебыприреёбы динаскотЖxwЮторуaô» счеспоноетали гла нидн ла, нурк пт, по пов1Огие ой», ть с\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XinV8nmAnmKN"
      },
      "source": [
        "## The mathematical trick in self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukiH-NbRBhA",
        "outputId": "241ee94a-9255-49ba-907c-daac171c6211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_E24uRE8kr",
        "outputId": "17770bb7-15e6-40ff-d2ef-143af5695a24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86NuXX0fn7ps"
      },
      "outputs": [],
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "3aecb34e-83e8-404f-baa6-85f2abf1c129"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "wOURrfG-ysoL",
        "outputId": "200f6f26-1726-41f9-bc8a-7494d81d8804"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (8x8 and 32x100)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-1d180189a231>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwei\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtril\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwei\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mxbow3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwei\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxbow3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8x8 and 32x100)"
          ]
        }
      ],
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "119af8c6-594d-494f-b72c-5d60ef474cb3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT1hdtzXCjgL",
        "outputId": "91b95018-c480-4ed1-e8de-1fb91715b6b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5CvobiQ0pLr"
      },
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SNbLq5z3oBw"
      },
      "outputs": [],
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6I9n9IRTSo",
        "outputId": "13d3fdd2-6284-4d8b-fa6c-4411b7656b00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "k.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tQx7oeRvtc",
        "outputId": "49520ad9-0600-4ed7-ac23-bb9d822576d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "q.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLb_odHU3iKM",
        "outputId": "4fdcf75f-29a7-44a0-f8c3-8b1f27a9344a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "wei.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "c0d69a5f-ec50-4cb5-d95b-150d2d57b7ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpt8569BB9_f",
        "outputId": "4d157b45-2d34-4cb2-82e5-48a2feb4690a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "cb4717a0-c9d2-46b7-ee65-3ec87483e711"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633T2cmnW1uk",
        "outputId": "53ad6bb8-399a-42f8-ce26-c346b175e680"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9cK9BoXCYb",
        "outputId": "e626e9fe-8a5d-4e1f-cc28-5b3e8ec569e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRJH6wM_XFfU"
      },
      "outputs": [],
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      },
      "source": [
        "### Full finished code, for reference\n",
        "\n",
        "You may want to refer directly to the git repo instead though."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/tolstoy-ln-ss22-03 (1).txt'"
      ],
      "metadata": {
        "id": "s-9a6sejtGV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "c01941e7-135f-4807-b667-7dcb5083ec8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.221081 M parameters\n",
            "step 0: train loss 5.2171, val loss 5.2278\n",
            "step 100: train loss 2.7037, val loss 2.9701\n",
            "step 200: train loss 2.5617, val loss 2.8653\n",
            "step 300: train loss 2.4710, val loss 2.8192\n",
            "step 400: train loss 2.3737, val loss 2.7454\n",
            "step 500: train loss 2.2926, val loss 2.7087\n",
            "step 600: train loss 2.2147, val loss 2.6534\n",
            "step 700: train loss 2.1705, val loss 2.5946\n",
            "step 800: train loss 2.1126, val loss 2.5685\n",
            "step 900: train loss 2.0695, val loss 2.5302\n",
            "step 1000: train loss 2.0434, val loss 2.5220\n",
            "step 1100: train loss 2.0105, val loss 2.4715\n",
            "step 1200: train loss 1.9794, val loss 2.4581\n",
            "step 1300: train loss 1.9462, val loss 2.4331\n",
            "step 1400: train loss 1.9254, val loss 2.4126\n",
            "step 1500: train loss 1.9117, val loss 2.4053\n",
            "step 1600: train loss 1.8917, val loss 2.3967\n",
            "step 1700: train loss 1.8719, val loss 2.3846\n",
            "step 1800: train loss 1.8571, val loss 2.3620\n",
            "step 1900: train loss 1.8369, val loss 2.3680\n",
            "step 2000: train loss 1.8303, val loss 2.3465\n",
            "step 2100: train loss 1.8172, val loss 2.3274\n",
            "step 2200: train loss 1.7981, val loss 2.3191\n",
            "step 2300: train loss 1.7914, val loss 2.3098\n",
            "step 2400: train loss 1.7782, val loss 2.3344\n",
            "step 2500: train loss 1.7786, val loss 2.3148\n",
            "step 2600: train loss 1.7642, val loss 2.3202\n",
            "step 2700: train loss 1.7572, val loss 2.3118\n",
            "step 2800: train loss 1.7504, val loss 2.2853\n",
            "step 2900: train loss 1.7379, val loss 2.2971\n",
            "step 3000: train loss 1.7368, val loss 2.2884\n",
            "step 3100: train loss 1.7305, val loss 2.2698\n",
            "step 3200: train loss 1.7214, val loss 2.2784\n",
            "step 3300: train loss 1.7196, val loss 2.2710\n",
            "step 3400: train loss 1.7117, val loss 2.2782\n",
            "step 3500: train loss 1.7080, val loss 2.2653\n",
            "step 3600: train loss 1.7042, val loss 2.2682\n",
            "step 3700: train loss 1.6987, val loss 2.2609\n",
            "step 3800: train loss 1.6900, val loss 2.2660\n",
            "step 3900: train loss 1.6871, val loss 2.2537\n",
            "step 4000: train loss 1.6890, val loss 2.2600\n",
            "step 4100: train loss 1.6887, val loss 2.2635\n",
            "step 4200: train loss 1.6782, val loss 2.2505\n",
            "step 4300: train loss 1.6716, val loss 2.2349\n",
            "step 4400: train loss 1.6722, val loss 2.2575\n",
            "step 4500: train loss 1.6708, val loss 2.2407\n",
            "step 4600: train loss 1.6630, val loss 2.2291\n",
            "step 4700: train loss 1.6574, val loss 2.2361\n",
            "step 4800: train loss 1.6663, val loss 2.2258\n",
            "step 4900: train loss 1.6535, val loss 2.2264\n",
            "step 4999: train loss 1.6438, val loss 2.2220\n",
            "\n",
            "Поликно; но это, – кричал он в дроме нельзя смеоти самотой говорил. Табаной, что тоужества тихо пробовы крылья.\n",
            "\n",
            "– Пьяно Ерошка хотелось переменут, и, стрелает окна в крымк, выходили рогами в Андрюменник; фрылями потремение, кто я на него, бывающий любовь действетсящий траком, нешал взяла ему глуфыом, шел дверь страницу. Они весело жихиться, пропорохлезно мальчишкими свящаную. – Что хотел захатах коновь по тонке ничего неовнидки из ли пятными, как блесловий дам, умаясь, вдруг отделать их. Сонесу. В жихой.\n",
            "\n",
            "– Сейдs. Дурятилась себе сучак! – сказала я, так казаках, обередний д?\n",
            "\n",
            "И он замертело, след, – сказала с оконственным ну,\n",
            "\n",
            "– Вы бутылуй быть все смоеркамие от двороизом брамное в себе казаки.\n",
            "\n",
            "– Я все рукаха биже! было я соней надила ее ми подругою солдатой. Уж такое малый зелеи. Только вышли были подшел за гремых былу.\n",
            "\n",
            "– Я говоряя тойти полку, Андрю. Куда в всем, – чесмотнец хозяич гловой, перементые, я понимает играли повторям, встароился в там, мне собщись. Ванюшка вниматься.\n",
            "\n",
            "И неспрездеть у любовь, я прося как я тома, бы совсе…\n",
            "\n",
            "Мне седенно за камышатенья?\n",
            "\n",
            "– – Тать, сказал труда, разгобычаривая мильчиковет как упину, судела на устощинна пили над долго и село. Резко который пустяком казалось, как и, не одлившие на самобой видны молодили товенья. Да вонисикие бы была залажившись и весело даст, жалко. Хроша тут себе, господи. Матушка впошла бургновкий мы сни сказал этой смех церкий, хотите есть умной забывал>но – просистащите молчатья.\n",
            "\n",
            "– Что, бограмился; на старика бедите, могла молодыха. Когда только в то крестилые в их. Чем гремчитое за его только и ты комнаты, донял, а мой одинатрельно надобно, у такие они волосы, с гребены, была видеть, – да Саты, кропку ними за кругими, он слушать его в собеннаются, – проговорил Хостати при только и ты золен?»\n",
            "\n",
            "– Нетжались! – сказал он.\n",
            "\n",
            "– Буду!» Хуже и хотел мало-то этот светку,\n",
            "\n",
            "– Право,\n",
            "\n",
            "Чему своега графиковски худом пришел, а всего еще, ветр там, чтобы кончан, ты столько сама потом я не спросил.\n",
            "\n",
            "– Ты заковыпыв трус\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64\n",
        "block_size = 32\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.1  # добавим dropout, чтобы не переобучалась\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def apply_rope(self, x):\n",
        "        B, T, C = x.shape\n",
        "        half = C // 2\n",
        "        x1 = x[..., :half]\n",
        "        x2 = x[..., half:]\n",
        "\n",
        "        pos = torch.arange(T, device=x.device).unsqueeze(-1)\n",
        "        freqs = 1.0 / (10000 ** (torch.arange(half, device=x.device).float() / half))\n",
        "        freqs = pos * freqs\n",
        "\n",
        "        sin = torch.sin(freqs).unsqueeze(0)\n",
        "        cos = torch.cos(freqs).unsqueeze(0)\n",
        "\n",
        "        x1_rot = x1 * cos - x2 * sin\n",
        "        x2_rot = x1 * sin + x2 * cos\n",
        "        return torch.cat([x1_rot, x2_rot], dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        q = self.apply_rope(self.query(x))\n",
        "        k = self.apply_rope(self.key(x))\n",
        "        wei = q @ k.transpose(-2, -1) * C ** -0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        x = tok_emb  # позиции кодируются через RoPE\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "model = model.to(device)\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# генерация\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "g1GyAQeSspET",
        "outputId": "bb8fe896-1669-4ea3-a04f-0238c2b43576",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.219033 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "snFXHL4qDRfp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}